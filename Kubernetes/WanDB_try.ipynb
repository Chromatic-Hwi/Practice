{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5ba775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.12.14-py2.py3-none-any.whl (1.8 MB)\n",
      "Requirement already satisfied: six>=1.13.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from wandb) (1.16.0)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.2.3-cp39-cp39-win_amd64.whl (10 kB)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\82108\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from wandb) (2.8.2)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from wandb) (2.26.0)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.10-py2.py3-none-any.whl (144 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\82108\\anaconda3\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.4)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.7)\n",
      "Building wheels for collected packages: promise, pathtools\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=7bca03d9f160be3b6c6574afa42ac1a33ce7277c6dea5fa54533fcd4be12391c\n",
      "  Stored in directory: c:\\users\\82108\\appdata\\local\\pip\\cache\\wheels\\e1\\e8\\83\\ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "  Building wheel for pathtools (setup.py): started\n",
      "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=6224148b6ec5bacde24dface263b9b0fc582a0ae72961f0f5f4c8e33e8f7a2bd\n",
      "  Stored in directory: c:\\users\\82108\\appdata\\local\\pip\\cache\\wheels\\b7\\0a\\67\\ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built promise pathtools\n",
      "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, promise, pathtools, GitPython, docker-pycreds, wandb\n",
      "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 promise-2.3 sentry-sdk-1.5.10 setproctitle-1.2.3 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\82108\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1858d26",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_276/2096827453.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mnist\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import wandb\n",
    "\n",
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "\n",
    "\n",
    "def input(dataset):\n",
    "    return dataset.images, dataset.labels.astype(np.int32)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Specify feature\n",
    "    feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[28, 28])]\n",
    "\n",
    "    # Build 2 layer DNN classifier\n",
    "    # NOTE: We change the summary logging frequency to be every epoch with save_summary_steps\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        hidden_units=[256, 32],\n",
    "        optimizer=tf.train.AdamOptimizer(1e-4),\n",
    "        n_classes=10,\n",
    "        dropout=0.1,\n",
    "        config=tf.estimator.RunConfig(\n",
    "            save_summary_steps=mnist.train.images.shape[0] / wandb.config.batch_size)\n",
    "    )\n",
    "\n",
    "    # Turn on logging\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    # Define the training inputs\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": input(mnist.train)[0]},\n",
    "        y=input(mnist.train)[1],\n",
    "        num_epochs=None,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # Train the classifier\n",
    "    classifier.train(input_fn=train_input_fn, steps=wandb.config.max_steps)\n",
    "\n",
    "    # Define the test inputs\n",
    "    test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": input(mnist.test)[0]},\n",
    "        y=input(mnist.test)[1],\n",
    "        num_epochs=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "    print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--max_steps', type=int, default=100000,\n",
    "                        help='Number of steps to run trainer.')\n",
    "    args = parser.parse_args()\n",
    "    wandb.init(tensorboard=True)\n",
    "    wandb.config.batch_size = 256\n",
    "    wandb.config.max_steps = args.max_steps\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a7f3fed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.examples'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_276/3210245251.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples'"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "\n",
    "\n",
    "def main():\n",
    "    wandb.init()\n",
    "\n",
    "    # Import Fashion MNIST data\n",
    "    data = input_data.read_data_sets('data/fashion')\n",
    "\n",
    "    categories = {\n",
    "        0: 'T-shirt/Top',\n",
    "        1: 'Trouser',\n",
    "        2: 'Pullover',\n",
    "        3: 'Dress',\n",
    "        4: 'Coat',\n",
    "        5: 'Sandal',\n",
    "        6: 'Shirt',\n",
    "        7: 'Sneaker',\n",
    "        8: 'Bag',\n",
    "        9: 'Ankle Boot'}\n",
    "\n",
    "    flags = tf.app.flags\n",
    "    flags.DEFINE_string('data_dir', '/tmp/data',\n",
    "                        'Directory with the mnist data.')\n",
    "    flags.DEFINE_integer('batch_size', 128, 'Batch size.')\n",
    "    flags.DEFINE_float('learning_rate', 0.1, 'Learning rate')\n",
    "\n",
    "    flags.DEFINE_integer('num_steps', 50000,\n",
    "                         'Num of batches to train.')\n",
    "    flags.DEFINE_integer('display_step', 100,\n",
    "                         'Steps between displaying output.')\n",
    "    flags.DEFINE_integer('n_hidden_1', 256, '1st layer number of neurons.')\n",
    "    flags.DEFINE_integer('n_hidden_2', 256, '2nd layer number of neurons.')\n",
    "    flags.DEFINE_integer(\n",
    "        'num_input', 784, 'MNIST data input (img shape: 28*28)')\n",
    "    flags.DEFINE_integer('num_classes', 10, 'MNIST total classes (0-9 digits)')\n",
    "\n",
    "    FLAGS = flags.FLAGS\n",
    "\n",
    "    # Import all of the tensorflow flags into wandb\n",
    "    wandb.config.update(FLAGS)\n",
    "\n",
    "    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, FLAGS.num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, FLAGS.num_classes])\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([FLAGS.num_input, FLAGS.n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([FLAGS.n_hidden_1, FLAGS.n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([FLAGS.n_hidden_2, FLAGS.num_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([FLAGS.n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([FLAGS.n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([FLAGS.num_classes]))\n",
    "    }\n",
    "\n",
    "    # Create model\n",
    "    def neural_net(x):\n",
    "        # Hidden fully connected layer with 256 neurons\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        # Hidden fully connected layer with 256 neurons\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        # Output fully connected layer with a neuron for each class\n",
    "        out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "        return out_layer\n",
    "\n",
    "    # Construct model\n",
    "    logits = neural_net(X)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start training\n",
    "    with tf.Session() as sess:\n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "\n",
    "        for step in range(1, FLAGS.num_steps+1):\n",
    "            batch_x, batch_y = mnist.train.next_batch(FLAGS.batch_size)\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "            if step % FLAGS.display_step == 0 or step == 1:\n",
    "                # Calculate batch loss and accuracy\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                     Y: batch_y})\n",
    "                val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={\n",
    "                    X: mnist.test.images,\n",
    "                    Y: mnist.test.labels})\n",
    "\n",
    "                print(\"Step \" + str(step) + \", Minibatch Loss= \" +\n",
    "                      \"{:.4f}\".format(loss) + \", Training Accuracy= \" +\n",
    "                      \"{:.3f}\".format(acc))\n",
    "\n",
    "                wandb.log({'acc': acc, 'loss': loss,\n",
    "                           'val_acc': acc, 'val_loss': val_loss})\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb -q \n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d04da7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\82108\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n",
      "wandb: ERROR Find detailed error logs at: C:\\Users\\82108\\AppData\\Local\\Temp\\debug-cli.log\n",
      "Error: api_key not configured (no-tty). call wandb login [your_api_key]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\82108/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\82108\\Documents\\GitHub\\Practice\\Kubernetes\\wandb\\run-20220417_123007-1241n7v8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/chromatic-hwi/wandb-tutorial/runs/1241n7v8\" target=\"_blank\">olive-brook-1</a></strong> to <a href=\"https://wandb.ai/chromatic-hwi/wandb-tutorial\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   1/1875 [..............................] - ETA: 37:53 - loss: 2.5218 - accuracy: 0.0000e+00WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0018s). Check your callbacks.\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3171 - accuracy: 0.9075 - _timestamp: 1650166235.0000 - _runtime: 28.0000\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2426 - accuracy: 0.9326 - _timestamp: 1650166239.0000 - _runtime: 32.0000\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2220 - accuracy: 0.9405 - _timestamp: 1650166244.0000 - _runtime: 37.0000\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2178 - accuracy: 0.9426 - _timestamp: 1650166249.0000 - _runtime: 42.0000\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2092 - accuracy: 0.9456 - _timestamp: 1650166253.0000 - _runtime: 46.0000\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1725 - accuracy: 0.9583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17250484228134155, 0.958299994468689]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb \n",
    "wandb.init(project=\"wandb-tutorial\", \n",
    "           config={ \"layer_1_activation\":\"relu\", \"layer_1\":128, \n",
    "                   \"learning_rate\":0.01, \n",
    "                   \"dropout_rate\": 0.2 }) \n",
    "\n",
    "config = wandb.config \n",
    "\n",
    "from wandb.keras import WandbCallback \n",
    "import tensorflow as tf \n",
    "minst = tf.keras.datasets.mnist \n",
    "(train_x, train_y), (test_x, test_y) = minst.load_data() \n",
    "# 데이터 전처리 \n",
    "train_x, test_x = train_x / 255.0, test_x / 255.0 \n",
    "# model settings \n",
    "model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), \n",
    "                                    tf.keras.layers.Dense(config.layer_1, \n",
    "                                                          activation=config.layer_1_activation), \n",
    "# 기존에는 128이라는 직접적인 숫자를 넣어주었지만, 이제는 config.layer_1이라고 수정합니다. \n",
    "tf.keras.layers.Dropout(config.dropout_rate), tf.keras.layers.Dense(10, activation=\"softmax\") ]) \n",
    "# 기존에는 optimizer=\"adam\", 기재하였으나 이는 learning_rate를 직접 지정할 수 없습니다. \n",
    "optimzier=tf.keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=config.learning_rate), \n",
    "              loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) \n",
    "model.fit(train_x, train_y, epochs=5, callbacks=[WandbCallback()]) \n",
    "model.evaluate(test_x, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
